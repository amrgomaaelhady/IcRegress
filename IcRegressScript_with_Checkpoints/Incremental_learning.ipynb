{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c028b150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import shutil\n",
    "\n",
    "# from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sklearn.model_selection\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error \n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from pytorchtools import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22d38a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,x,y,training_type):\n",
    "\n",
    "        self.x_train=torch.tensor(x,dtype=torch.float32)\n",
    "        if training_type == 'classify':\n",
    "            self.y_train=torch.tensor(y,dtype=torch.int64)\n",
    "        elif training_type == 'regress':\n",
    "            self.y_train=torch.tensor(y,dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y_train)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.x_train[idx],self.y_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a9801bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetCNN(nn.Module):\n",
    "    def __init__(self,dim_num,classes):\n",
    "        super(NetCNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(dim_num, 64, kernel_size=2),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            nn.Dropout(p=0.3)\n",
    "        )\n",
    "        self.layer1_1 = nn.Sequential(\n",
    "            nn.Conv1d(64, 16, kernel_size=2),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            nn.Dropout(p=0.3)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv1d(16, 8, kernel_size=3),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            nn.Dropout(p=0.3)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(8, 48),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3)\n",
    "        )\n",
    "        \n",
    "        self.layer3_1 = nn.Sequential(\n",
    "            nn.Linear(48, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3)\n",
    "        )\n",
    "        \n",
    "        self.layer4 = nn.Linear(in_features=20, out_features=classes)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer1_1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)   # Flatten them for FC\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer3_1(out)\n",
    "        out = self.layer4(out)\n",
    "\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef6d2dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetLSTM(nn.Module):\n",
    "    def __init__(self,dim_num,classes):\n",
    "        super(NetLSTM, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.LSTM(input_size=dim_num,\n",
    "                            hidden_size=16,\n",
    "                            num_layers=3,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        \n",
    "        self.layer4 = nn.Linear(in_features=640, out_features=classes)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out, states = self.layer1(x)\n",
    "        out = out.reshape(out.size(0), -1)   # Flatten them for Many to one LSTM\n",
    "        out = self.layer4(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2794dd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetTransf(nn.Module):\n",
    "    def __init__(self, dim_num: int, d_model: int, nhead: int, d_hid: int, \n",
    "                 nlayers: int, classes: int, dropout: float, batch_size: int):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "#         self.encoder = nn.Linear(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "#         self.dim_num = dim_num\n",
    "        self.decoder = nn.Linear(dim_num, classes)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "#         self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        src = src * math.sqrt(self.d_model)\n",
    "#         print(src.shape)\n",
    "        src = src.permute(2, 0, 1)\n",
    "#         print(src.shape)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "#         print(output.shape)\n",
    "        output = output.mean(0)\n",
    "#         print(output.shape)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "    \n",
    "# def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "#     \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "#     return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81441cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_to_run(data_train, data_test, input_columns, mode, training_type, number_epochs, batch_size, learning_rate, fine_tuning,fine_tune_ckp_path, inference_only, inference_ckp_path, regress_type):\n",
    "\n",
    "    X_train = data_train[input_columns]\n",
    "    X_test = data_test[input_columns]\n",
    "    \n",
    "    if training_type == 'classify':\n",
    "        y_train = data_train.POIBuilding_new\n",
    "        y_test = data_test.POIBuilding_new\n",
    "        output_classes = 16\n",
    "        \n",
    "    elif training_type == 'regress':\n",
    "        y_train = data_train[['PoI_GT_centerLatAngle',\n",
    "                      'PoI_GT_visibleMinLatAngle','PoI_GT_visibleMaxLatAngle','PoI_GT_minLatAngle',\n",
    "                      'PoI_GT_maxLatAngle','PoI_GT_adjustedMinLatAngle','PoI_GT_adjustedMaxLatAngle']]\n",
    "        y_test = data_test[['PoI_GT_centerLatAngle',\n",
    "                    'PoI_GT_visibleMinLatAngle','PoI_GT_visibleMaxLatAngle','PoI_GT_minLatAngle',\n",
    "                    'PoI_GT_maxLatAngle','PoI_GT_adjustedMinLatAngle','PoI_GT_adjustedMaxLatAngle']]\n",
    "        output_classes = 1\n",
    "\n",
    "    if mode == 'CNN' or mode == 'Transformer':\n",
    "        X_train = (X_train.values.reshape(X_train.shape[0],X_train.shape[-1]//20,20))#.swapaxes(1,2)\n",
    "        X_test = (X_test.values.reshape(X_test.shape[0],X_test.shape[-1]//20,20))#.swapaxes(1,2)\n",
    "    \n",
    "    elif mode == 'LSTM':\n",
    "        X_train = (X_train.values.reshape(X_train.shape[0],X_train.shape[-1]//20,20)).swapaxes(1,2)\n",
    "        X_test = (X_test.values.reshape(X_test.shape[0],X_test.shape[-1]//20,20)).swapaxes(1,2)\n",
    "    \n",
    "    print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "    \n",
    "    \n",
    "    X_train, X_val, y_train, y_val = sklearn.model_selection.train_test_split(\n",
    "       X_train, y_train, random_state=42, test_size=0.12\n",
    "    )\n",
    "    \n",
    "    print(X_train.shape, X_val.shape, X_test.shape, y_train.shape, y_val.shape, y_test.shape)\n",
    "    print(\"#\"*80)\n",
    "    \n",
    "    if mode == 'Transformer':\n",
    "        ntokens = X_train.shape[1]  # size of vocabulary\n",
    "        emsize = 8  # embedding dimension\n",
    "        d_hid = 50  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "        nlayers = 6  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "        nhead = 8  # number of heads in nn.MultiheadAttention\n",
    "        dropout = 0.2  # dropout probability\n",
    "        net = NetTransf(ntokens, emsize, nhead, d_hid, nlayers, output_classes, dropout, batch_size)\n",
    "\n",
    "    if mode == 'CNN':\n",
    "        net = NetCNN(X_train.shape[1],classes=output_classes)\n",
    "    \n",
    "    elif mode == 'LSTM':\n",
    "        net = NetLSTM(X_train.shape[-1],classes=output_classes)\n",
    "    \n",
    "    print(net)\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "    print(device)\n",
    "    # In[17]:\n",
    "    net.to(device)\n",
    "\n",
    "\n",
    "    def save_ckp(state, is_best,f_path):\n",
    "        torch.save(state, f_path)\n",
    "        if is_best:\n",
    "            best_fpath = 'best_model.pt'\n",
    "            shutil.copyfile(f_path, best_fpath)\n",
    "\n",
    "    def load_ckp(checkpoint_fpath, model, optimizer):\n",
    "        checkpoint = torch.load(checkpoint_fpath)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        return model, optimizer, checkpoint['epoch']\n",
    "    \n",
    "    #padding nan values in X\n",
    "#     X_train = np.nan_to_num(X_train)\n",
    "#     X_test = np.nan_to_num(X_test)\n",
    "#     X_val = np.nan_to_num(X_val)\n",
    "    \n",
    "    \n",
    "    if training_type == 'classify':\n",
    "        y_train = y_train.values.reshape(-1,1)\n",
    "        y_val = torch.squeeze(torch.tensor(y_val.values.reshape(-1,1),dtype=torch.int64)).to(device)\n",
    "        dataset = MyDataset(X_train, y_train,training_type)\n",
    "        \n",
    "    elif training_type == 'regress' and regress_type == 'regress_center':\n",
    "        print('regress_center')\n",
    "        y_train_center_only = y_train.PoI_GT_centerLatAngle.values.reshape(-1,1)\n",
    "        y_val_center_only = torch.tensor(y_val.PoI_GT_centerLatAngle.values.reshape(-1,1),dtype=torch.float32).to(device)\n",
    "        dataset = MyDataset(X_train, y_train_center_only,training_type)\n",
    "    \n",
    "    elif regress_type == 'regress_facade':\n",
    "        print('regress_facade')\n",
    "        y_train_center_only = (y_train[['PoI_GT_visibleMinLatAngle','PoI_GT_visibleMaxLatAngle']].mean(axis=1)).values.reshape(-1,1)\n",
    "        y_val_center_only = torch.tensor((y_val[['PoI_GT_visibleMinLatAngle','PoI_GT_visibleMaxLatAngle']].mean(axis=1)).values.reshape(-1,1),dtype=torch.float32).to(device)\n",
    "        dataset = MyDataset(X_train, y_train_center_only,training_type)\n",
    "\n",
    "    #create dataset and generate dataloader\n",
    "    \n",
    "    data_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, \n",
    "                             collate_fn=lambda x: tuple(x_.to(device) for x_ in default_collate(x)))\n",
    "\n",
    "    # Define Optimizer and Loss Function\n",
    "#     optimizer = torch.optim.SGD(net.parameters(), lr=0.2)\n",
    "    optimizer = torch.optim.Adam(params=net.parameters(), lr=learning_rate)\n",
    "    \n",
    "\n",
    "    \n",
    "    if training_type == 'classify':\n",
    "        loss_func = torch.nn.CrossEntropyLoss()\n",
    "    elif training_type == 'regress':\n",
    "        loss_func = torch.nn.MSELoss()\n",
    "    \n",
    "    #saving checkpoints for continuing training (e.g., transfer learning)\n",
    "    \n",
    "    if not inference_only:\n",
    "    \n",
    "        if fine_tuning:\n",
    "            net, optimizer, start_epoch = load_ckp(fine_tune_ckp_path, net, optimizer)\n",
    "            print(start_epoch)\n",
    "\n",
    "        print('Training the Deep Learning network ...')\n",
    "\n",
    "\n",
    "\n",
    "        val_losses = []\n",
    "        train_losses = []\n",
    "\n",
    "\n",
    "        # initialize the early_stopping object\n",
    "        early_stopping = EarlyStopping(patience=40, verbose=True)\n",
    "\n",
    "\n",
    "\n",
    "        for epoch in range(number_epochs):\n",
    "            print(epoch)\n",
    "            avg_cost = []\n",
    "\n",
    "            net.train()\n",
    "            for i, (batch_X, batch_Y) in enumerate(data_loader):\n",
    "                X = Variable(batch_X)    \n",
    "                Y = Variable(batch_Y)\n",
    "                if training_type == 'classify':\n",
    "                    Y = torch.squeeze(Y)\n",
    "\n",
    "                optimizer.zero_grad() # <= initialization of the gradients\n",
    "\n",
    "                # forward propagation\n",
    "                prediction = net(X)\n",
    "    #             print(prediction.shape, Y.shape)\n",
    "                loss = loss_func(prediction, Y) # <= compute the loss function\n",
    "\n",
    "                # Backward propagation\n",
    "                loss.backward() # <= compute the gradient of the loss/cost function     \n",
    "                optimizer.step() # <= Update the gradients\n",
    "\n",
    "\n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': net.state_dict(),\n",
    "                'optimizer': optimizer.state_dict()\n",
    "            }\n",
    "\n",
    "            if fine_tuning:\n",
    "                save_ckp(checkpoint, False, 'checkpoint_finetuned.pt')\n",
    "            else:\n",
    "                save_ckp(checkpoint, False, 'checkpoint.pt')\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "    #         print('train loss:', loss.item())\n",
    "\n",
    "            net.eval() # prep model for evaluation\n",
    "            with torch.no_grad():\n",
    "    #             print(X_val)\n",
    "                X_val_trch = torch.tensor(X_val,dtype=torch.float32).to(device)\n",
    "    #             print(X_val_trch.shape)\n",
    "                prediction_val = net(X_val_trch)\n",
    "    #             print(prediction_val,y_val_center_only)\n",
    "                if training_type == 'classify':\n",
    "                    loss_val = loss_func(prediction_val, y_val)\n",
    "                elif training_type == 'regress':\n",
    "                    loss_val = loss_func(prediction_val, y_val_center_only)\n",
    "                val_losses.append(loss_val.item())\n",
    "    #             print('val loss:', loss_val.item())\n",
    "\n",
    "            # early_stopping needs the validation loss to check if it has decresed, \n",
    "            # and if it has, it will make a checkpoint of the current model\n",
    "            early_stopping(np.average(val_losses), net)\n",
    "            print(np.average(val_losses))\n",
    "            if early_stopping.early_stop:\n",
    "                save_ckp(checkpoint, True, 'checkpoint_earlystopping_full.pt')\n",
    "                print(\"Early stopping at Epoch \" + str(epoch))\n",
    "\n",
    "        print('Learning Finished!')\n",
    "\n",
    "        print('last train loss:', loss.item())\n",
    "        print('last val loss:', loss_val.item())\n",
    "\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.title(\"Training and Validation Loss\")\n",
    "        plt.plot(val_losses,label=\"val\")\n",
    "        plt.plot(train_losses,label=\"train\")\n",
    "        plt.xlabel(\"iterations\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "#     if inference_only:\n",
    "#         net.load_state_dict(torch.load(inference_ckp_path))\n",
    "#         net.eval()\n",
    "        \n",
    "    if inference_only:\n",
    "        \n",
    "        if \"earlystop\" in inference_ckp_path:\n",
    "            net.load_state_dict(torch.load(inference_ckp_path))\n",
    "        else:\n",
    "            net, optimizer, start_epoch = load_ckp(inference_ckp_path, net, optimizer)\n",
    "            print(start_epoch)\n",
    "            \n",
    "        net.eval()\n",
    "    \n",
    "    X_train_trch = torch.from_numpy(X_train).float().to(device)\n",
    "    np.savetxt('X_train.csv',X_train.reshape(X_train.shape[0], -1), delimiter=\",\")\n",
    "    print(X_train_trch.shape)\n",
    "    with torch.no_grad():\n",
    "        prediction_train = net(X_train_trch).cpu()\n",
    "    \n",
    "    if training_type == 'classify':\n",
    "        softmax = torch.exp(prediction_train).cpu()\n",
    "        prob = list(softmax.numpy())\n",
    "        predictions = np.argmax(prob, axis=1)\n",
    "\n",
    "        # accuracy on training set\n",
    "        final_accuracy = accuracy_score(y_train, predictions)\n",
    "        print('Train Accuracy: ', final_accuracy)\n",
    "\n",
    "    elif training_type == 'regress':\n",
    "        print(prediction_train.shape)\n",
    "        prediction_np = prediction_train.detach().numpy().reshape(len(prediction_train))\n",
    "        \n",
    "        y_train['PoI_GT_centerLatAngle_predicted'] = prediction_np\n",
    "        \n",
    "        #generate list of indices for incremental learning, use K when loading to use\n",
    "        print(\"(((((((())))))))\")\n",
    "        print(y_train.columns)\n",
    "        y_train['prediction_error'] = (y_train['PoI_GT_centerLatAngle'] - y_train['PoI_GT_centerLatAngle_predicted']).abs()\n",
    "        temp_y_train = y_train.sort_values(by='prediction_error').reset_index(names='priority_index')\n",
    "\n",
    "        # uncomment the following if you want to save a new priotirized examplers and move the file to the Data folder\n",
    "#         temp_y_train['priority_index'].to_csv('y_train_indeces_priotirized.csv')\n",
    "        print(\"(((((((())))))))\")\n",
    "        \n",
    "        final_accuracy = len(y_train[(y_train.PoI_GT_centerLatAngle_predicted > y_train.PoI_GT_minLatAngle) \n",
    "               & (y_train.PoI_GT_centerLatAngle_predicted < y_train.PoI_GT_maxLatAngle)]) / len(y_train)\n",
    "        print('Train Accuracy MRDE: ', final_accuracy)\n",
    "\n",
    "        final_accuracy = len(y_train[(y_train.PoI_GT_centerLatAngle_predicted > y_train.PoI_GT_visibleMinLatAngle) \n",
    "               & (y_train.PoI_GT_centerLatAngle_predicted < y_train.PoI_GT_visibleMaxLatAngle)]) / len(y_train)\n",
    "        print('Train Accuracy SegObj: ', final_accuracy)\n",
    "\n",
    "        final_accuracy = len(y_train[(y_train.PoI_GT_centerLatAngle_predicted > y_train.PoI_GT_adjustedMinLatAngle) \n",
    "               & (y_train.PoI_GT_centerLatAngle_predicted < y_train.PoI_GT_adjustedMaxLatAngle)]) / len(y_train)\n",
    "        print('Train Accuracy MinDT: ', final_accuracy)\n",
    "\n",
    "    X_val_trch = torch.from_numpy(X_val).float().to(device)\n",
    "    print(X_val_trch.shape)\n",
    "    with torch.no_grad():\n",
    "        prediction_val = net(X_val_trch).cpu()\n",
    "    \n",
    "    if training_type == 'classify':\n",
    "        softmax = torch.exp(prediction_val).cpu()\n",
    "        prob = list(softmax.numpy())\n",
    "        predictions = np.argmax(prob, axis=1)\n",
    "\n",
    "        # accuracy on training set\n",
    "        final_accuracy = accuracy_score(y_val, predictions)\n",
    "        print('Val Accuracy: ', final_accuracy)\n",
    "\n",
    "    elif training_type == 'regress':\n",
    "        print(prediction_val.shape)\n",
    "        prediction_np = prediction_val.detach().numpy().reshape(len(prediction_val))\n",
    "\n",
    "        y_val['PoI_GT_centerLatAngle_predicted'] = prediction_np\n",
    "        final_accuracy = len(y_val[(y_val.PoI_GT_centerLatAngle_predicted > y_val.PoI_GT_minLatAngle) \n",
    "               & (y_val.PoI_GT_centerLatAngle_predicted < y_val.PoI_GT_maxLatAngle)]) / len(y_val)\n",
    "        print('Val Accuracy MRDE: ', final_accuracy)\n",
    "\n",
    "        final_accuracy = len(y_val[(y_val.PoI_GT_centerLatAngle_predicted > y_val.PoI_GT_visibleMinLatAngle) \n",
    "               & (y_val.PoI_GT_centerLatAngle_predicted < y_val.PoI_GT_visibleMaxLatAngle)]) / len(y_val)\n",
    "        print('Val Accuracy SegObj: ', final_accuracy)\n",
    "\n",
    "        final_accuracy = len(y_val[(y_val.PoI_GT_centerLatAngle_predicted > y_val.PoI_GT_adjustedMinLatAngle) \n",
    "               & (y_val.PoI_GT_centerLatAngle_predicted < y_val.PoI_GT_adjustedMaxLatAngle)]) / len(y_val)\n",
    "        print('Val Accuracy MinDT: ', final_accuracy)\n",
    "\n",
    "\n",
    "    X_test_trch = torch.from_numpy(X_test).float().to(device)\n",
    "    print(X_test_trch.shape)\n",
    "    with torch.no_grad():\n",
    "        prediction_test = net(X_test_trch).cpu()\n",
    "        \n",
    "        \n",
    "    if training_type == 'classify':\n",
    "        softmax = torch.exp(prediction_test).cpu()\n",
    "        prob = list(softmax.numpy())\n",
    "        predictions = np.argmax(prob, axis=1)\n",
    "\n",
    "        # accuracy on test set\n",
    "        final_accuracy_test = accuracy_score(y_test, predictions)\n",
    "        print('Test Accuracy: ', final_accuracy_test)\n",
    "        \n",
    "        return final_accuracy_test\n",
    "\n",
    "    elif training_type == 'regress':\n",
    "        print(prediction_test.shape)\n",
    "        prediction_test_np = prediction_test.detach().numpy().reshape(len(prediction_test))\n",
    "        \n",
    "        y_test['PoI_GT_centerLatAngle_predicted'] = prediction_test_np\n",
    "        \n",
    "        performance_askl = sklearn.metrics.mean_squared_error(y_test[['PoI_GT_visibleMinLatAngle','PoI_GT_visibleMaxLatAngle']].mean(axis=1), y_test['PoI_GT_centerLatAngle_predicted'], squared=False)\n",
    "        print(f\"CNN Regressor MSE is {performance_askl}\")\n",
    "        print(math.degrees(performance_askl))\n",
    "    \n",
    "        final_accuracy_latangle = len(y_test[(y_test.PoI_GT_centerLatAngle_predicted > y_test.PoI_GT_minLatAngle) \n",
    "               & (y_test.PoI_GT_centerLatAngle_predicted < y_test.PoI_GT_maxLatAngle)]) / len(y_test)\n",
    "        print('Test Accuracy MRDE: ', final_accuracy_latangle)\n",
    "\n",
    "        final_accuracy_visiblelatangle = len(y_test[(y_test.PoI_GT_centerLatAngle_predicted > y_test.PoI_GT_visibleMinLatAngle) \n",
    "               & (y_test.PoI_GT_centerLatAngle_predicted < y_test.PoI_GT_visibleMaxLatAngle)]) / len(y_test)\n",
    "        print('Test Accuracy SegObj: ', final_accuracy_visiblelatangle)\n",
    "\n",
    "        final_accuracy_adjustedlatangle = len(y_test[(y_test.PoI_GT_centerLatAngle_predicted > y_test.PoI_GT_adjustedMinLatAngle) \n",
    "               & (y_test.PoI_GT_centerLatAngle_predicted < y_test.PoI_GT_adjustedMaxLatAngle)]) / len(y_test)\n",
    "        print('Test Accuracy MinDT: ', final_accuracy_adjustedlatangle)\n",
    "\n",
    "        return final_accuracy_latangle, final_accuracy_visiblelatangle, final_accuracy_adjustedlatangle\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6082f062",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"../Data/\"\n",
    "mode = 'CNN' #CNN or LSTM or Transformer network\n",
    "training_type = 'regress' #classify or regress\n",
    "regress_type = 'regress_center' #regress_facade or regress_center\n",
    "batch_size = 128\n",
    "number_epochs = 20000\n",
    "learning_rate = 0.0001\n",
    "fine_tune_or_not = False\n",
    "fine_tune_ckp_path = \"checkpoint.pt\"\n",
    "inference_only = True\n",
    "# inference_ckp_path = \"checkpoint.pt\"\n",
    "inference_ckp_path = \"checkpoint_earlystop.pt\"\n",
    "exemplar_ratio = 0 # 2 is half, 4 is quarter and so on while 0 is naive finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b21b71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_runs_concatenate(file_path1, file_path2 = False, exemplars_list_path = False):\n",
    "    \n",
    "    data = pd.read_csv(folder_path + file_path1).iloc[:,2:]\n",
    "    data['POIBuilding_new'] = data.POIBuilding_new - 1\n",
    "#     print(data)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #dropping test for the currently loaded variant and loading it from data2\n",
    "    data = data[~((data.Participant == 'P45') \n",
    "                      | (data.Participant == 'P75') \n",
    "                      | (data.Participant == 'P22') \n",
    "                      | (data.Participant == 'P40')\n",
    "                      | (data.Participant == 'P57')\n",
    "                      | (data.Participant == 'P74')\n",
    "                       )]\n",
    "    \n",
    "    if file_path2 != False:\n",
    "        print('A second File Path was given for the test Data.')\n",
    "        print(file_path2)\n",
    "        data2 = pd.read_csv(folder_path + file_path2).iloc[:,2:]\n",
    "        data2['POIBuilding_new'] = data2.POIBuilding_new - 1\n",
    "        \n",
    "        #left is 22,45,74\n",
    "        #right is 40,57,75\n",
    "        #lessthan4 is 40,74,75\n",
    "        #morethan6 is 22,45,57\n",
    "        \n",
    "        test_data_index_in_IsNoise = data2[(\n",
    "                        (data2.Participant == 'P45') \n",
    "                      | (data2.Participant == 'P75') \n",
    "                      | (data2.Participant == 'P22') \n",
    "                      | (data2.Participant == 'P40')\n",
    "                      | (data2.Participant == 'P57')\n",
    "                      | (data2.Participant == 'P74')\n",
    "                     )].index.tolist()\n",
    "        \n",
    "        \n",
    "        if exemplars_list_path != False:\n",
    "            exemplars_temp = pd.read_csv(folder_path + exemplars_list_path).iloc[:,1:]\n",
    "            \n",
    "            if exemplar_ratio != 0:\n",
    "                exemplar_number = len(exemplars_temp) // exemplar_ratio\n",
    "                exemplars_list = exemplars_temp['priority_index'].iloc[:exemplar_number].tolist()\n",
    "                print(len(exemplars_list))\n",
    "#                 exemplars_list2 = exemplars_temp['priority_index'].iloc[-exemplar_number:].tolist()\n",
    "#                 exemplars_list.extend(exemplars_list2)\n",
    "                exemplars_list.extend(test_data_index_in_IsNoise)\n",
    "                print(len(exemplars_list))\n",
    "            elif exemplar_ratio == 0:\n",
    "                exemplars_list = test_data_index_in_IsNoise\n",
    "                print(len(exemplars_list))\n",
    "                \n",
    "            print(exemplars_list)\n",
    "            data2 = data2.iloc[exemplars_list]\n",
    "            \n",
    "        \n",
    "        data = pd.concat([data,data2])\n",
    "        \n",
    "        \n",
    "    data.drop_duplicates(inplace=True)\n",
    "    data.reset_index(inplace=True)\n",
    "#     print(data)\n",
    "    #     print(data.columns)\n",
    "    print(\"#\"*80)\n",
    "\n",
    "    # ffill and bfill instead of replacing np.nan with zeros\n",
    "    data[[col for col in data.columns if 'PointingDirectionX_' in col]] = data[[col for col in data.columns if 'PointingDirectionX_' in col]].ffill(axis=1).bfill(axis=1)\n",
    "    data[[col for col in data.columns if 'PointingDirectionZ_' in col]] = data[[col for col in data.columns if 'PointingDirectionZ_' in col]].ffill(axis=1).bfill(axis=1)\n",
    "    data[[col for col in data.columns if 'HeadGazeDirectionX_' in col]] = data[[col for col in data.columns if 'HeadGazeDirectionX_' in col]].ffill(axis=1).bfill(axis=1)\n",
    "    data[[col for col in data.columns if 'HeadGazeDirectionZ_' in col]] = data[[col for col in data.columns if 'HeadGazeDirectionZ_' in col]].ffill(axis=1).bfill(axis=1)\n",
    "    data[[col for col in data.columns if 'HeadposeX_' in col]] = data[[col for col in data.columns if 'HeadposeX_' in col]].ffill(axis=1).bfill(axis=1)\n",
    "    data[[col for col in data.columns if 'HeadposeZ_' in col]] = data[[col for col in data.columns if 'HeadposeZ_' in col]].ffill(axis=1).bfill(axis=1)\n",
    "    data[[col for col in data.columns if 'GazeX_' in col]] = data[[col for col in data.columns if 'GazeX_' in col]].ffill(axis=1).bfill(axis=1)\n",
    "    data[[col for col in data.columns if 'GazeZ_' in col]] = data[[col for col in data.columns if 'GazeZ_' in col]].ffill(axis=1).bfill(axis=1)\n",
    "    data.dropna(inplace=True)\n",
    "    \n",
    "\n",
    "    data_train = data[~((data.Participant == 'P45') \n",
    "                      | (data.Participant == 'P75') \n",
    "                      | (data.Participant == 'P22') \n",
    "                      | (data.Participant == 'P40')\n",
    "                      | (data.Participant == 'P57')\n",
    "                      | (data.Participant == 'P74')\n",
    "                       )]\n",
    "    data_test = data[((data.Participant == 'P45') \n",
    "                      | (data.Participant == 'P75') \n",
    "                      | (data.Participant == 'P22') \n",
    "                      | (data.Participant == 'P40')\n",
    "                      | (data.Participant == 'P57')\n",
    "                      | (data.Participant == 'P74')\n",
    "                     )]\n",
    "    \n",
    "    input_columns = []\n",
    "    input_columns.extend([col for col in data_test.columns if ('PointingDirectionX_' in col) or ('PointingDirectionZ_' in col)])\n",
    "    input_columns.extend([col for col in data_test.columns if ('HeadGazeDirectionX_' in col) or ('HeadGazeDirectionZ_' in col)])\n",
    "    input_columns.extend([col for col in data_test.columns if ('HeadposeX_' in col) or ('HeadposeZ_' in col)])\n",
    "    input_columns.extend([col for col in data_test.columns if ('GazeX_' in col) or ('GazeZ_' in col)])\n",
    "\n",
    "#     input_columns = [col for col in data_test.columns if ('X_' in col) or ('Z_' in col)]\n",
    "\n",
    "#     print(len(input_columns))\n",
    "#     print(data_train.columns)\n",
    "    \n",
    "    \n",
    "    if training_type == 'classify':\n",
    "    \n",
    "        all_runs_acc = []\n",
    "\n",
    "\n",
    "        for i in range(1):\n",
    "            \n",
    "            final_accuracy = main_to_run(data_train,data_test,input_columns,mode,training_type,number_epochs,batch_size,learning_rate,fine_tune_or_not,fine_tune_ckp_path,inference_only, inference_ckp_path, regress_type)\n",
    "            all_runs_acc.append(final_accuracy)\n",
    "\n",
    "        print(\"#\"*80)\n",
    "        print(\"#\"*80)\n",
    "        print(all_runs_acc)\n",
    "        print((np.asarray(all_runs_acc)).mean())\n",
    "        print(\"#\"*80)\n",
    "        print(\"#\"*80)\n",
    "    \n",
    "    elif training_type == 'regress':\n",
    "#     else:\n",
    "        \n",
    "        all_runs_latangle = []\n",
    "        all_runs_visiblelatangle = []\n",
    "        all_runs_adjustedlatangle = []\n",
    "\n",
    "        for i in range(1):\n",
    "            \n",
    "            final_accuracy_latangle, final_accuracy_visiblelatangle, final_accuracy_adjustedlatangle = main_to_run(data_train,data_test,input_columns,mode,training_type,number_epochs,batch_size,learning_rate,fine_tune_or_not,fine_tune_ckp_path,inference_only, inference_ckp_path, regress_type)\n",
    "            \n",
    "            all_runs_latangle.append(final_accuracy_latangle)\n",
    "            all_runs_visiblelatangle.append(final_accuracy_visiblelatangle)\n",
    "            all_runs_adjustedlatangle.append(final_accuracy_adjustedlatangle)\n",
    "\n",
    "        print(\"#\"*80)\n",
    "        print(\"#\"*80)\n",
    "        print(all_runs_latangle, all_runs_visiblelatangle, all_runs_adjustedlatangle)\n",
    "        print((np.asarray(all_runs_latangle)).mean(), (np.asarray(all_runs_visiblelatangle)).mean() ,(np.asarray(all_runs_adjustedlatangle)).mean())\n",
    "        print(\"#\"*80)\n",
    "        print(\"#\"*80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b962b808",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A second File Path was given for the test Data.\n",
      "/Data_based_on_Speech_onset.csv\n",
      "################################################################################\n",
      "(3414, 8, 20) (275, 8, 20) (3414, 7) (275, 7)\n",
      "(3004, 8, 20) (410, 8, 20) (275, 8, 20) (3004, 7) (410, 7) (275, 7)\n",
      "################################################################################\n",
      "NetCNN(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv1d(8, 64, kernel_size=(2,), stride=(1,))\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (layer1_1): Sequential(\n",
      "    (0): Conv1d(64, 16, kernel_size=(2,), stride=(1,))\n",
      "    (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv1d(16, 8, kernel_size=(3,), stride=(1,))\n",
      "    (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=48, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (layer3_1): Sequential(\n",
      "    (0): Linear(in_features=48, out_features=20, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (layer4): Linear(in_features=20, out_features=1, bias=True)\n",
      ")\n",
      "cuda:0\n",
      "regress_center\n",
      "torch.Size([3004, 8, 20])\n",
      "torch.Size([3004, 1])\n",
      "(((((((())))))))\n",
      "Index(['PoI_GT_centerLatAngle', 'PoI_GT_visibleMinLatAngle',\n",
      "       'PoI_GT_visibleMaxLatAngle', 'PoI_GT_minLatAngle', 'PoI_GT_maxLatAngle',\n",
      "       'PoI_GT_adjustedMinLatAngle', 'PoI_GT_adjustedMaxLatAngle',\n",
      "       'PoI_GT_centerLatAngle_predicted'],\n",
      "      dtype='object')\n",
      "(((((((())))))))\n",
      "Train Accuracy MRDE:  0.7017310252996005\n",
      "Train Accuracy SegObj:  0.47569906790945404\n",
      "Train Accuracy MinDT:  0.5382822902796272\n",
      "torch.Size([410, 8, 20])\n",
      "torch.Size([410, 1])\n",
      "Val Accuracy MRDE:  0.675609756097561\n",
      "Val Accuracy SegObj:  0.44146341463414634\n",
      "Val Accuracy MinDT:  0.5219512195121951\n",
      "torch.Size([275, 8, 20])\n",
      "torch.Size([275, 1])\n",
      "CNN Regressor MSE is 0.16395382238146997\n",
      "9.393862057495765\n",
      "Test Accuracy MRDE:  0.730909090909091\n",
      "Test Accuracy SegObj:  0.3709090909090909\n",
      "Test Accuracy MinDT:  0.4218181818181818\n",
      "################################################################################\n",
      "################################################################################\n",
      "[0.730909090909091] [0.3709090909090909] [0.4218181818181818]\n",
      "0.730909090909091 0.3709090909090909 0.4218181818181818\n",
      "################################################################################\n",
      "################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_185071/4268575643.py:317: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y_test['PoI_GT_centerLatAngle_predicted'] = prediction_test_np\n"
     ]
    }
   ],
   "source": [
    "multiple_runs_concatenate('Data_based_on_gesture_onset.csv','/Data_based_on_Speech_onset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9025b12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
